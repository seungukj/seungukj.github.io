---
title: 'What to say for IVT or EVT classes'
date: 2022-11-27
permalink: /posts/2022/11/calculus-ivt-evt/
author_profile: false
tags:
  - English
  - Calculus
  - Mathematical Pedagogy
  - Intermediate Value Theorem
  - Extreme Value Theorem
---

This note is to record some tales that may be adequate while introducing Intermediate Value Theorem (IVT) and Extreme Value Theorem (EVT) for continuous functions. Some ideas behind the proofs, a relation with long division algorithm, etc. are introduced.

# Backgrounds

Many calculus texts (Stewart, Thomas, ...) introduce the following theorems of continuous functions.

 > **Intermediate Value Theorem.** Let $f\colon[a,b]\to\mathbb{R}$ be a continuous function, and let $K\in\mathbb{R}$ be a number between $f(a)$ and $f(b)$. Then there exists $c\in[a,b]$ such that $f(c)=K$.

 > **Extreme Value Theorem.** Let $f\colon[a,b]\to\mathbb{R}$ be a continuous function. Then the values $M=\sup_{x\in[a,b]}f(x)$ and $m=\inf_{x\in[a,b]}f(x)$ are attained as function values. That is, there exists $c_1,c_2\in[a,b]$ such that $f(c_1)=M$ and $f(c_2)=m$.

If we understand continuous functions as a 'function whose graph can be sketched with a continous path of pencil stroke,' each theorem has an evident pictorial significance:

 * Intermediate value theorem says, if we want to connect $(a,f(a))$ and $(b,f(b))$ by a continuous path, one necessarily pass through the horizontal line line $y=K$ (provided that $K$ is between $f(a),f(b)$).
 * Extreme value theorem says, if we want to connect $(a,f(a))$ and $(b,f(b))$ by a continuous path, one cannot go infinitely 'up,' nor infinitely 'down.'

Furthermore, both theorems helps us to fill up unconscious gaps that one can make while thinking about continuous or differentiable functions. For instance, in optimization problems of a differentiable function $f\colon[a,b]\to\mathbb{R}$, solving $f'(x)=0$ *does* help us finding global extrema. This is purely because there exists a global extrema: solving $f'(x)=0$ can list all local extrema, but checking whether one of them is a global extrema needs a separate argument to guarantee so.

It seems like there are not much further one can say about the theorem, other than speaking of examples and nonexamples on the statement of the theorem. But because both theorems are nonconstructive in its statement, so to fully appreciate them one may need to be able to appreciate mathematical abstraction and significance.

My understanding is that appreciating mathematical abstraction and significance is sometimes (very often?) a foreign culture for participants of calculus courses. To massage such foreignness, I would note some 'practical math' that can explain both theorems: **hashing the continuous function.**

# Hashing

For simplicity, suppose $[a,b]=[0,1]$. (Replace $f(x)$ to $\hat{f}(t)=f\left(a+(b-a)t\right)$ if necessary.) Fix an integer $N>0$ and consider the following data.

$$f\left(\frac{k}{2^N}\right),\quad k=0,1,\ldots,2^N.$$

This array of $2^N+1$ numbers will be called the **$N$-th (binary) hashing** of the function $f$. The array elements will be called **hashing elements.** It is immediate that elements of the $N$-th hashing are elements of the $(N+1)$-th hashing.

If $f\colon[0,1]\to\mathbb{R}$ is a (uniformly) continuous function, then hashings are good approximations of $f$ in the following sense.

 > <b id="hashing-approx">Proposition.</b> Suppose $f\colon[0,1]\to\mathbb{R}$ is (uniformly) continuous. For any $\epsilon>0$, there exists an integer $N>0$ such that for every $x\in[0,1]$, whenever $\vert x-k/2^N\vert \leq 2^{-N}$, we have
 >
 > $$f(k/2^N)-\epsilon < f(x) < f(k/2^N)+\epsilon.$$

That is, every function value $f(x)$ is less than $\epsilon$ away from some hashing element in some binary hashing.

Hashing a continuous function allows us to (approximately) view a function as a finite array of function values. This allows us to replace the questions like

 * For a number $K$, can we solve $f(x)=K$?
 * Does $f$ attain global maximum or minimum?

to discrete versions like

 * For a number $K$, is it appearing as a hashing element? (Or reasonably close to it?)
 * Given a hashing, do we have maximal or minimal hashing elements?

# Extreme Value Theorem

Given the $N$-th binary hashing, it is obvious that we have maximal and minimal hashing elements; just scan through the numbers and record the maximum or minimum. For sake of analysis, name $c_N$ (respectively, $d_N$) for the point $k/2^N$ where $f(k/2^N)$ is maximum (respectively, minimum) among $k=0,1,\ldots,2^N$.

What gets interesting is the phenomena as $N\to\infty$. Because the $(N+1)$-th hashing embraces the $N$-th hashing as a subset, the maximum hashing element gets bigger and the minimum hashing element gets smaller as $N$ increases. That is,

$$f\left(c_{N+1}\right)\geq f\left(c_N\right); \\ f\left(d_{N+1}\right)\leq f\left(d_N\right).$$

One can raise the following questions.

 * Will $$f(c_N)$$'s keep increase and achieve $\sup f([0,1])$ at the limit $N\to\infty$?
 * Will $$c_N$$'s head to the global maximum?

We state the dual questions, for sake of completeness:

 * Will $$f(d_N)$$'s keep decrease and achieve $\inf f([0,1])$ at the limit $N\to\infty$?
 * Will $$d_N$$'s head to the global minimum?

The first question is affirmative as stated, and the second question needs some fix to be affirmative. Stated formally, we have:

 > **Extreme Value Theorem** (with hashing). Let $f,c_N$ be as above.
 >
 > 1. We have $$\left(f(c_N)\right)_{N=1}^\infty$$ an increasing sequence, whose limit is same as $\sup_{x\in[0,1]}f(x)$.
 > 1. Any limit point $x$ of the sequence $$(c_N)$$ is a global maximum.

It turns out that both facts result from continuity, and one can construct examples where the theorem fails without continuity. (Simply let the global maximum be attained at an irrational point whose value cannot be approximated by hashing elements.)

But what is even more interesting is as follows.

 * Fact 1. above holds even if the domain is a bounded set.
 * Fact 2. above holds even if the domain is a closed set.

Although standard topology classes view Extreme Value Theorem as a result of compactness, this hashing-based 'algorithm' assigns an own significance to closedness and boundedness of the domain. Of course introducing limit points or closedness are typically out of the scope of regular calculus courses. But it is worth keeping them in mind (as an instructor) because one can attribute topology for examples and nonexamples of the EVT.

## Sample Lecture Plan

 > <font style="font-style:normal">One of the key properties of continuous function defined on a closed and bounded interval, is that one can find an <i>absolute</i> maximum and minimum of the function. To elaborate, we first define some terms.</font>
 >> **Definition.** Given a (real-valued) function $f(x)$, we say a point $x=x_0$ (in the domain) is an *absolute maximum* if $f(x_0)\geq f(x)$ holds for all $x$ in the domain; *absolute minimum* if $f(x_0)\leq f(x)$ holds for all $x$ in the domain.
 >
 > <font style="font-style:normal">Then the <b>Extreme Value Theorem</b> is stated as follows.</font>
 >
 >> **Extreme Value Theorem.** Let $f\colon[a,b]\to\mathbb{R}$ be a continuous function defined on a closed and bounded interval. Then the function $f$ has an absolute maximum and an absolute minimum.
 >
 > <font style="font-style:normal">The theorem itself does not tell on <i>how</i> to find absolute maximum or minimum; it only tells there is one. Perhaps one can list all possible candidates (using derivatives) and find one that works the best; but to guarantee that the absolute maximum or minimum is within these candidates, we first need to know that they <i>exist</i>. The theorem is guaranteeing such a fact.</font>
 >
 > <font style="font-style:normal">Although we will later work on methods using derivatives to find absolute (or local) maxima or minima, it is still interesting to introduce a method - to find absolute maxima or minima - that applies for all continuous functions, differentiable or not. The method is called the <b>hashing method</b>.</font>
 >
 > <font style="font-style:normal">The idea is this. Given a continuous function $f\colon[0,1]\to\mathbb{R}$, suppose we record the function as a list $F[k]:=f(k/2^N)$. (Or more generally, $F[k]=f(a+(b-a)k/2^N)$ if f is defined on $[a,b]$.) This array $F[0],F[1],\ldots,F[2^N]$ of length $2^N+1$ is a reasonable approximate representation of the function. Indeed, for any $x\in[0,1]$, if we take $k/2^N$ closest to $x$ then one can have $f(x)$ arbitrarily close to $F[k]=f(k/2^N)$ (by setting $N$ big enough; justifying it requires some advanced notion of continuity, however).</font>
 >
 > ![sample hashing](/images/221127-fig1.png)
 >
 > <font style="font-style:normal">We call the array $F[0],F[1],\ldots,F[2^N]$ the <b>$N$-th binary hashing</b> of the function $f(x)$, and call elements of this array as <b>hashing elements</b>. This may be seen as a way to record our function $f(x)$ within a computer.</font>
 >
 > <font style="font-style:normal">If we have a list of numbers, we can obviously find its maximum within that list. Furthermore, <i>that maximum in the hashing well-approximates an absolute maximum, too.</i> By this note, one can propose the following 'algorithm' to find an absolute maximum.</font>
 >
 > * Make the $N$-th binary hashing, with $N$ large.
 > * Find a maximum hashing element; call it $$F[k_N]=f(c_N)$$ (i.e., $$c_N=k_N/2^N$$).
 > * Increase $N$ over and over. The maximum hash element $$F[k_N]$$ then converges to the absolute maximum value of $f$, and points $$c_N$$ on the domain approximates an absolute maximum point of $f$.
 >
 > ![hashing-max algorithm](/images/221127-fig2.png)
 >
 > <font style="font-style:normal">This method of finding an absolute maximum of a continuous function is based on a fairly straightforward algorithm of finding the maximum of a finite list of numbers. However, it has clear drawbacks: once we have a good guess where an absolute maximum is, most of the hashing elements are unnecessary but still appearing in algorithms. Methods using derivatives are good ways to get rid of such inefficiency.</font>
 >
 >> **Remark.** <font style="font-style:normal">One warning about the maximum point $c_N$ within the hashing is that, the points $$c_1,c_2,\ldots$$ may oscillate between two points.</font>
 >> ![oscillating-max](/images/221127-fig3.png)
 >> <font style="font-style:normal">This is typically the case when there are more than one absolute maximum points. In that case the maximum hashing element $$F[k_N]=f(c_N)$$ may be found at multiple places, causing the oscillation of concern. However, choosing one 'place' (or 'region') of oscillation suffices to find an absolute maximum.
 >
 > <font style="font-style:normal">There are few cases where the algorithm does not work as intuitively understood. Often this is caused by dropping continuity or dropping conditions of the domain - closed and bounded.</font>
 >
 > 1. <font style="font-style:normal">When the maximum hashing elements suggest an absolute maximum that is not really an absolute maximum. See (a) below.</font>
 > 1. <font style="font-style:normal">When we keep increase $f(c_N)$'s and reach to infinities. See (b) below.</font>
 > 1. <font style="font-style:normal">When the domain is unbounded so that $c_N$'s are not even defined. See (c) below.</font>
 >
 > ![anomalies](/images/221127-fig4.png)
 >
 > <font style="font-style:normal">We should thus understand that the assumptions that (1) the function has domain as a closed interval, and (2) the function is continuous, as assumptions that are designed for the algorithm to work in a proper way.</font>
 >
 > <font style="font-style:normal">It is left as exercise to think of how absolute minima is found by the hashing method.</font>

### Acknowledgement

This idea of hashing a continuous function and application to the Extreme Value Theorem came from a personal communication with [Junekey Jeon](https://jk-jeon.github.io/).

# Intermediate Value Theorem

With the method of hashing, intermediate value theorem is ought to be 'proven' by the bisection method. Here we keep our assumption $[a,b]=[0,1]$ here; we further assume $K=0$ and $f(0)<0<f(1)$. (Replace $f(x)\leftarrow K-f(x)$ or $f(x)\leftarrow f(x)-K$ if necessary.)

Standard proof of the Intermediate Value Theorem starts by taking the following supremum.

$$c=\sup\{x\in[0,1] : f(x)\leq 0\}.$$

Then one challenges to show that $f(c)=0$ from this setup.

This proof has a straightforward translation in terms of the hashing method. Simply take the nonpositive hashing element $f(k/2^N)$ where $k=0,1,\ldots,2^N$ is set as big as possible. The sequence $c_N=k/2^N$ obtained is an increasing sequence whose limit is a zero of $f(x)$. (More precisely, $c_N\to c$.)

However, proving that $c=\lim c_N$ is a zero of $f(x)$ requires some smart use of the squeeze method, as sketched below.

 > <font style="font-style:normal">Suppose $f(c)\neq 0$. Set $\epsilon=\frac12\vert f(c)\vert$ and choose $N$ according to a <a href="#hashing-approx">Proposition</a> above. By the definition of $c_N$, we have $c_N=\lfloor 2^Nc\rfloor/2^N$ and $f(c_N)\leq 0<f(c_N+2^{-N})$.</font>
 >
 > <font style="font-style:normal">If $f(c)<0$, then $0<f(c_N+2^{-N})<f(c)+\epsilon=\frac12 f(c)<0$ gives a contradiction. If $f(c)>0$, then $0\geq f(c_N)>f(c)-\epsilon=\frac12f(c)>0$ gives a contradiction.</font>

Perhaps building up this proof can be an interesting exercise for an analysis course. But it seems like this proof is out of the scope of a regular calculus course. Nonetheless, stating that the hashing method still gives an idea to prove IVT, seems worth introducing.

## Bisection method

One consequence of the Intermediate Value Theorem is the **bisection method,** a numerical algorithm to find a zero of a continuous function. The method is aimed to 'squeeze' the possible range $[a_N,b_N]$ of finding a zero. To formally write this as a form of algorithm, we say:

 1. Make sure that $f(\frac12)\neq 0$ and $f(0)<0<f(1)$.
 1. Set $(a_1,b_1)=(0,\frac12,1)$.
 1. For each $N=1,2,3,\cdots$, define $(a_{N+1},b_{N+1})$ by the following case-by-case basis.
     1. If $f(\frac12(a_N+b_N))=0$, return $c=\frac12(a_N+b_N)$ and terminate.
     1. If $f(\frac12(a_N+b_N))<0$, set $(a_{N+1},b_{N+1})=(\frac12(a_N+b_N),b_N)$.
     1. If $f(\frac12(a_N+b_N))>0$, set $(a_{N+1},b_{N+1})=(a_N,\frac12(a_N+b_N))$.
 1. If the above procedure does not terminate, set $c=\lim_{N\to\infty}c_N$.

One can show that the pair $(a_{N+1},b_{N+1})$ appears as consecutive elements in the list $0/2^N,1/2^N,\ldots,2^N/2^N$. Thus the method is somewhat relevant to the hashing method. Furthermore, the method gives a proof of Intermediate Value Theorem in the following sense.

 > **Intermediate Value Theorem** (by bisection method). Let $c$ be the number found above. Then we have $f(c)=0$.

However, the proof is again requiring some analysis. One needs (i) the notion of Cauchy sequence to ensure that the limit $c=\lim c_N$ exists, and (ii) some smart use of the uniform continuity to apply 'squeeze method' and obtain $f(c)=0$.

## Decasection method

Now the bisection method is most likely to be a foreign method for average students. One way to convince that the method is something that they have already seen is to upgrade this to a 'deca-section method,' meaning that we update $(a_N,b_N)$ by

 * $a_{N+1}=a_N+q\cdot 10^{-N}$ and $b_{N+1}=a_N+(q+1)\cdot 10^{-N}$, where $q=0,1,\ldots,9$ is a number such that $f(a_N+q\cdot 10^{-N})\leq 0\leq f(a_N+(q+1)\cdot 10^{-N})$.

(This method can be conveyed better if one has a picture that subdivides $[a_N,b_N]$ into 10 equal pieces and choose one that passes through a zero of $f(x)$.)

This can be still new for the version stated above, but if one thinks of a division problem, say $13\div 9$, then setting $f(x)=9x-13$ one has $f(0)=-13<0<f(10)=77$, and updates $(a_1,b_1)=(0,10)$ as follows.

 * Since $f(1)=-4<0<f(2)=5$, we update $(a_2,b_2)=(1,2)$;
 * since $f(1.4)=-0.4<0<f(1.5)=0.5$, we update $(a_3,b_3)=(1.4,1.5)$;
 * since $f(1.44)=-0.04<0<f(1.45)=0.05$, we update $(a_4,b_4)=(1.44,1.45)$,

etc. It is now evident that we are essentially running the long division algorithm, but stated in the calculus language. Even, this will be a moment that students see that the long division algorithm (that they may encountered around a decade ago) is mathematically justified. If possible, then one can leverage this to appreciate the beauty of mathematical rigor or abstractions.

This decasection method not only justifies the traditional (numerical) division algorithm, but also applies to find zeroes of polynomials numerically. For instance, one can ask to compute $\sqrt{0.5}$ by applying decasection method to $f(x)=x^2-0.5$. Computations are typically painful in this direction, but one can obtain $\sqrt{0.5}=0.707\ldots$ just before seeing some heavy numbers.

#### Update Log
 * <span style="font-size:12px">221127: Created</span>
